# Benchmark Datasets on Knowledge-based Machine Reading Comprehension.

<!--A list of recent papers about **Knowledge-based Machine Reading Comprehension** (**KMRC**).-->

Contributed by [Luxi Xing](https://github.com/XingLuxi), [Yuqiang Xie](https://github.com/IndexFziQ) and [Wei Peng](https://github.com/a414351664).

Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China. 

Update on **Aug. 12, 2020**.

(We will continuously update this list.)

----

1. [**COPA**] Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI,2011. [[paper](http://ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF) / [data](http://people.ict.usc.edu/~gordon/copa.html)]
    
    Authors: *Melissa Roemmele, Cosmin Adrian Bejan, Andrew S. Gordon*
    * Type: Multiple-Choice;
    
1. [**WSC**] The Winograd Schema Challenge. AAAI,2011. [[paper](https://www.aaai.org/ocs/index.php/KR/KR12/paper/download/4492/4924) /[data](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)]
    
    Authors: *Hector J. Levesque, Ernest Davis, Leora Morgenstern*
    * Type: Multiple-Choice;

2. [**ROCStories**; **SCT**] A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. NAACL,2016. [[paper](https://www.aclweb.org/anthology/N16-1098/) / [data](https://www.cs.rochester.edu/nlp/rocstories/)]
    
    Authors: *Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen*
    * Type: Cloze;
    
1. [**NarrativeQA**] The NarrativeQA Reading Comprehension Challenge. TACL,2018. [[paper](https://arxiv.org/abs/1712.07040) / [data](https://github.com/deepmind/narrativeqa)]
    
    Authors: *Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette*
    * Type: Generation;
    
1. [**SemEval-2018 Task 11**] MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge. LERC,2018. [[paper](https://arxiv.org/abs/1803.05223) / [data](http://www.sfb1102.uni-saarland.de/?page_id=2582)]
    
    Authors: *Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, Manfred Pinkal*
    
    * Type: Multiple-Choice;
    
1. [**story-commonsense**] Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018. [[paper](https://www.aclweb.org/anthology/P18-1213/) / [data](http://uwnlp.github.io/storycommonsense)]
    
    Authors: *Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, Yejin Choi*
    
    * Type: Multiple-Choice;
    
1. **Event2Mind**: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. [[paper](https://www.aclweb.org/anthology/P18-1043/) / [data](https://uwnlp.github.io/event2mind/)]
    
    Authors: *Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, Yejin Choi*
    
    * Types: Generation;

1. **ATOMIC**: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019. [[paper](https://homes.cs.washington.edu/~msap/atomic/data/sap2019atomic.pdf) / [data](https://homes.cs.washington.edu/~msap/atomic/)]
    
    Authors: *Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, Yejin Choi*
    
    * Types: Generation;

1. [**ARC**] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018. [[paper](http://ai2-website.s3.amazonaws.com/publications/AI2ReasoningChallenge2018.pdf) / [data](http://data.allenai.org/arc/)]
    
    Authors: *Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord*
  
    * Type: Multiple-Choice;
    
1. [**OpenBookQA**] Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. EMNLP,2018. [[paper](https://www.aclweb.org/anthology/D18-1260/) / [data](https://leaderboard.allenai.org/open_book_qa)]
    
    Authors: *Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal*

    * Type: Multiple-Choice;
    
1. **ReCoRD**: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. 2018. [[paper](https://arxiv.org/abs/1810.12885) / [data](https://sheng-z.github.io/ReCoRD-explorer/)]
    
    Authors: *Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme*
    
    * Type: Cloze;
    
1. **CommonsenseQA**: A Question Answering Challenge Targeting Commonsense Knowledge. NAACL,2019. [[paper](https://www.aclweb.org/anthology/N19-1421/) / [data](https://www.tau-nlp.org/commonsenseqa)]
    
    Authors: *Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant*
    
    * Type: Multiple-Choice;
    
1. **ChID**: A Large-scale Chinese IDiom Dataset for Cloze Test. ACL,2019. [[paper](https://www.aclweb.org/anthology/P19-1075) / [data](https://github.com/chujiezheng/ChID-Dataset)]
    
    Authors: *Chujie Zheng, Minlie Huang, Aixin Sun*

    * Type: Cloze;
    
1. [**sense-making**] Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation. ACL,2019. [[paper](https://www.aclweb.org/anthology/P19-1393/) / [data]( https://github.com/wangcunxiang/SenMaking-and-Explanation)]
    
    Authors: *Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao*
    
    * Type: Multiple-Choice;
    
1. **HellaSwag**: Can a Machine Really Finish Your Sentence? ACL,2019. [[paper](https://arxiv.org/abs/1905.07830) / [data](https://rowanzellers.com/hellaswag/)]
    
    Authors: *Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi*

    * Type: Multiple-Choice;
    
1. **SocialIQA**: Commonsense Reasoning about Social Interactions. EMNLP,2019. [[paper](https://arxiv.org/abs/1904.09728) / [data](https://maartensap.github.io/social-iqa/)]
    
    Authors: *Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi*

    * Type: Multiple-Choice;

1. [**ANLI**] Abductive Commonsense Reasoning. 2019. [[paper](https://arxiv.org/abs/1908.05739) / [data](https://leaderboard.allenai.org/anli/submissions/get-started)]
    
    Authors: *Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, Yejin Choi*

    * Type: Multiple-Choice;

1. **Cosmos QA**: Machine Reading Comprehension with Contextual Commonsense Reasoning. EMNLP,2019. [[paper](https://arxiv.org/abs/1909.00277) / [data](https://wilburone.github.io/cosmos/)]
    
    Authors: *Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi*

    * Type: Multiple-Choice;

1. **CODAH**: An Adversarially-Authored Question Answering Dataset for Common Sense. ACL,2019,workshop. [[paper](https://www.aclweb.org/anthology/W19-2008) / [data](https://github.com/Websail-NU/CODAH) ]

    Authors: *Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fernandez, Doug Downey*
    
    * Type: Multiple-Choice;

1. **CommonGen**: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning. 2019. [ [paper](http://arxiv.org/abs/1911.03705) / [data](http://inklab.usc.edu/CommonGen/) ]

    Authors: *Bill Yuchen Lin, Ming Shen, Yu Xing, Pei Zhou, Xiang Ren*
    
    * Type: Generative;

1. **QASC**: A Dataset for Question Answering via Sentence Composition. 2019. [[paper](http://arxiv.org/abs/1910.11473) / [data](https://leaderboard.allenai.org/qasc) ]

    Authors: *Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal*
    
    * Type: Multiple-Choice;
    
1. **DROP**: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. NAACL,2019. [[paper](http://arxiv.org/abs/1903.00161)]

    Authros: *Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner*


Note: *Only consider the benchmark datasets/tasks which require knowledge to complete.*

<!--
| Publish | Dataset | Links | Domain | Task | Size | Other |
| :------ | :------ | :---: | :----- | :--: | :---:| :---- |
-->
