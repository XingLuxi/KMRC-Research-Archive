# Benchmark Datasets on Knowledge-based Machine Reading Comprehension.

<!--A list of recent papers about **Knowledge-based Machine Reading Comprehension** (**KMRC**).-->

![](https://img.shields.io/badge/Status-building-brightgreen) ![](https://img.shields.io/badge/-corpus-lightgray) 

Contributed by [Luxi Xing](https://github.com/XingLuxi), [Yuqiang Xie](https://github.com/IndexFziQ) and [Wei Peng](https://github.com/a414351664).

Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China. 

Update on **March 2, 2021**.

<!--(We will continuously update this list.)-->

----

1. [**COPA**] Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI,2011. [[paper](http://ict.usc.edu/~gordon/publications/AAAI-SPRING11A.PDF) / [data](http://people.ict.usc.edu/~gordon/copa.html)]
    
    Authors: *Melissa Roemmele, Cosmin Adrian Bejan, Andrew S. Gordon*
    * Type: Multiple-Choice;
    
1. [**WSC**] The Winograd Schema Challenge. AAAI,2011. [[paper](https://www.aaai.org/ocs/index.php/KR/KR12/paper/download/4492/4924) /[data](https://cs.nyu.edu/faculty/davise/papers/WinogradSchemas/WS.html)]
    
    Authors: *Hector J. Levesque, Ernest Davis, Leora Morgenstern*
    * Type: Multiple-Choice;

2. [**ROCStories**; **SCT**] A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories. NAACL,2016. [[paper](https://www.aclweb.org/anthology/N16-1098/) / [data](https://www.cs.rochester.edu/nlp/rocstories/)]
    
    Authors: *Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen*
    * Type: Cloze;
    
1. [**NarrativeQA**] The NarrativeQA Reading Comprehension Challenge. TACL,2018. [[paper](https://arxiv.org/abs/1712.07040) / [data](https://github.com/deepmind/narrativeqa)]
    
    Authors: *Tomáš Kočiský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, Edward Grefenstette*
    * Type: Generation;
    
1. [**SemEval-2018 Task 11**] MCScript: A Novel Dataset for Assessing Machine Comprehension Using Script Knowledge. LERC,2018. [[paper](https://arxiv.org/abs/1803.05223) / [data](http://www.sfb1102.uni-saarland.de/?page_id=2582)]
    
    Authors: *Simon Ostermann, Ashutosh Modi, Michael Roth, Stefan Thater, Manfred Pinkal*
    
    * Type: Multiple-Choice;
    
1. [**story-commonsense**] Modeling Naive Psychology of Characters in Simple Commonsense Stories. ACL,2018. [[paper](https://www.aclweb.org/anthology/P18-1213/) / [data](http://uwnlp.github.io/storycommonsense)]
    
    Authors: *Hannah Rashkin, Antoine Bosselut, Maarten Sap, Kevin Knight, Yejin Choi*
    
    * Type: Multiple-Choice;
    
1. **Event2Mind**: Commonsense Inference on Events, Intents, and Reactions. ACL,2018. [[paper](https://www.aclweb.org/anthology/P18-1043/) / [data](https://uwnlp.github.io/event2mind/)]
    
    Authors: *Hannah Rashkin, Maarten Sap, Emily Allaway, Noah A. Smith, Yejin Choi*
    
    * Types: Generation;

1. **ATOMIC**: An Atlas of Machine Commonsense for If-Then Reasoning. AAAI,2019. [[paper](https://homes.cs.washington.edu/~msap/atomic/data/sap2019atomic.pdf) / [data](https://homes.cs.washington.edu/~msap/atomic/)]
    
    Authors: *Maarten Sap, Ronan LeBras, Emily Allaway, Chandra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A. Smith, Yejin Choi*
    
    * Types: Generation;

1. [**ARC**] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. 2018. [[paper](http://ai2-website.s3.amazonaws.com/publications/AI2ReasoningChallenge2018.pdf) / [data](http://data.allenai.org/arc/)]
    
    Authors: *Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord*
  
    * Type: Multiple-Choice;
    
1. [**OpenBookQA**] Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. EMNLP,2018. [[paper](https://www.aclweb.org/anthology/D18-1260/) / [data](https://leaderboard.allenai.org/open_book_qa)]
    
    Authors: *Todor Mihaylov, Peter Clark, Tushar Khot, Ashish Sabharwal*

    * Type: Multiple-Choice;
    
1. **ReCoRD**: Bridging the Gap between Human and Machine Commonsense Reading Comprehension. 2018. [[paper](https://arxiv.org/abs/1810.12885) / [data](https://sheng-z.github.io/ReCoRD-explorer/)]
    
    Authors: *Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, Benjamin Van Durme*
    
    * Type: Cloze;
    
1. **CommonsenseQA**: A Question Answering Challenge Targeting Commonsense Knowledge. NAACL,2019. [[paper](https://www.aclweb.org/anthology/N19-1421/) / [data](https://www.tau-nlp.org/commonsenseqa)]
    
    Authors: *Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant*
    
    * Type: Multiple-Choice;
    
1. **ChID**: A Large-scale Chinese IDiom Dataset for Cloze Test. ACL,2019. [[paper](https://www.aclweb.org/anthology/P19-1075) / [data](https://github.com/chujiezheng/ChID-Dataset)]
    
    Authors: *Chujie Zheng, Minlie Huang, Aixin Sun*

    * Type: Cloze;
    
1. [**sense-making**] Does it Make Sense? And Why? A Pilot Study for Sense Making and Explanation. ACL,2019. [[paper](https://www.aclweb.org/anthology/P19-1393/) / [data]( https://github.com/wangcunxiang/SenMaking-and-Explanation)]
    
    Authors: *Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao*
    
    * Type: Multiple-Choice;
    
1. **HellaSwag**: Can a Machine Really Finish Your Sentence? ACL,2019. [[paper](https://arxiv.org/abs/1905.07830) / [data](https://rowanzellers.com/hellaswag/)]
    
    Authors: *Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi*

    * Type: Multiple-Choice;
    
1. **SocialIQA**: Commonsense Reasoning about Social Interactions. EMNLP,2019. [[paper](https://arxiv.org/abs/1904.09728) / [data](https://maartensap.github.io/social-iqa/)]
    
    Authors: *Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, Yejin Choi*

    * Type: Multiple-Choice;

1. [**ANLI**] Abductive Commonsense Reasoning. 2019. [[paper](https://arxiv.org/abs/1908.05739) / [data](https://leaderboard.allenai.org/anli/submissions/get-started)]
    
    Authors: *Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, Yejin Choi*

    * Type: Multiple-Choice;

1. **Cosmos QA**: Machine Reading Comprehension with Contextual Commonsense Reasoning. EMNLP,2019. [[paper](https://arxiv.org/abs/1909.00277) / [data](https://wilburone.github.io/cosmos/)]
    
    Authors: *Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, Yejin Choi*

    * Type: Multiple-Choice;

1. **CODAH**: An Adversarially-Authored Question Answering Dataset for Common Sense. ACL,2019,workshop. [[paper](https://www.aclweb.org/anthology/W19-2008) / [data](https://github.com/Websail-NU/CODAH) ]

    Authors: *Michael Chen, Mike D’Arcy, Alisa Liu, Jared Fernandez, Doug Downey*
    
    * Type: Multiple-Choice;

1. **CommonGen**: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning. 2019. [ [paper](http://arxiv.org/abs/1911.03705) / [data](http://inklab.usc.edu/CommonGen/) ]

    Authors: *Bill Yuchen Lin, Ming Shen, Yu Xing, Pei Zhou, Xiang Ren*
    
    * Type: Generative;

1. **QASC**: A Dataset for Question Answering via Sentence Composition. 2019. [[paper](http://arxiv.org/abs/1910.11473) / [data](https://leaderboard.allenai.org/qasc) ]

    Authors: *Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal*
    
    * Type: Multiple-Choice;
    
1. **DROP**: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. NAACL,2019. [[paper](http://arxiv.org/abs/1903.00161)]

    Authors: *Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner*

    * Type: Multi-type
    * Metrics: EM/F1

1. **WIQA**: A dataset for "What if ..." reasoning over procedural text. EMNLP,2019. [[paper](http://arxiv.org/abs/1909.04739)]

    * Type: Multiple-Choice

1. [**ROPEs**] Reasoning Over Paragraph Effects in Situations. ACL,2019,workshop. [[paper](https://www.aclweb.org/anthology/D19-5808)]

    * Type: Multiple-Choice

1. **R^3**: A Reading Comprehension Benchmark Requiring Reasoning Process. 2020. [[paper](http://arxiv.org/abs/2004.01251)]

2. **ProtoQA**: A Question Answering Dataset for Prototypical Common-Sense Reasoning. EMNLP,2020. [[paper](http://arxiv.org/abs/2005.00771)]

3. **ESPRIT**: Explaining Solutions to Physical Reasoning Tasks. ACL,2020. [[paper](https://www.aclweb.org/anthology/2020.acl-main.706)]

4. **R4C**: A Benchmark for Evaluating RC Systems to Get the Right Answer for the Right Reason. ACL,2020. [[paper](https://www.aclweb.org/anthology/2020.acl-main.602)]

5. **LogiQA**: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning. IJCAI,2020. [[paper](http://arxiv.org/abs/2007.08124)]
 
6. **MathQA**: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. NAACL-HIT,2019. [[paper](https://www.aclweb.org/anthology/N19-1245)]

7. **QuaRel**: A Dataset and Models for Answering Questions about Qualitative Relationships. AAAI,2019. [[paper](http://arxiv.org/abs/1811.08048)]

8. **Quoref**: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning. EMNLP,2019. [[paper](http://arxiv.org/abs/1908.05803)]

9. **KILT**: a Benchmark for Knowledge Intensive Language Tasks. 2020. [[paper](https://arxiv.org/abs/2009.02252) / [data](http://kiltbenchmark.com/) / [code](https://github.com/facebookresearch/KILT)]

10. **QED**: A Framework and Dataset for Explanations in Question Answering. 2020. [[paper](http://arxiv.org/abs/2009.06354)]

11. **Learning to Explain**: Datasets and Models for Identifying Valid Reasoning Chains in Multihop Question-Answering. EMNLP,2020. [[paper](http://arxiv.org/abs/2010.03274)]

12. **QuaRTz**: An Open-Domain Dataset of Qualitative Relationship Questions. EMNLP,2019. [[paper](https://www.aclweb.org/anthology/D19-1608)]

13. **IIRC**: A Dataset of Incomplete Information Reading Comprehension Questions. EMNLP,2020. [[paper](https://www.aclweb.org/anthology/2020.emnlp-main.86)]

14. **TyDi QA**: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages. TACL,2020. [[paper](https://www.aclweb.org/anthology/2020.tacl-1.30)]

15. **TORQUE**: A Reading Comprehension Dataset of Temporal Ordering Questions. EMNLP,2020. [[paper](https://www.aclweb.org/anthology/2020.emnlp-main.88)]


1. [**StrategyQA**] Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. 2021. [[paper](http://arxiv.org/abs/2101.02235)]

    * Type: Yes/No (Boolean)


1. [**ARC-DA**] Think you have Solved Direct-Answer Question Answering? Try ARC-DA, the Direct-Answer AI2 Reasoning Challenge. 2021. [[paper](https://arxiv.org/abs/2102.03315v1)]

    * Type: Generation
 
 

Note: *Only consider the benchmark datasets/tasks which require knowledge to complete.*

<!--
| Publish | Dataset | Links | Domain | Task | Size | Other |
| :------ | :------ | :---: | :----- | :--: | :---:| :---- |
-->
